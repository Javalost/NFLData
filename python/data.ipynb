{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NFLVerse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nfl_data_py as nfl\n",
    "\n",
    "# Get play-by-play data for the 2023 NFL season\n",
    "pbp_data = nfl.import_pbp_data([2023])\n",
    "\n",
    "# Display the first few rows of the data\n",
    "print(pbp_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FootballDB Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved stats for passing to stat_tables/passing_stats.html\n",
      "Successfully saved stats for rushing to stat_tables/rushing_stats.html\n",
      "Successfully saved stats for receiving to stat_tables/receiving_stats.html\n",
      "Successfully saved stats for scoring to stat_tables/scoring_stats.html\n",
      "Successfully saved stats for kickoffreturns to stat_tables/kickoffreturns_stats.html\n",
      "Successfully saved stats for puntreturns to stat_tables/puntreturns_stats.html\n",
      "Successfully saved stats for punting to stat_tables/punting_stats.html\n",
      "Successfully saved stats for fieldgoals to stat_tables/fieldgoals_stats.html\n",
      "Successfully saved stats for interceptions to stat_tables/interceptions_stats.html\n",
      "Successfully saved stats for sacks to stat_tables/sacks_stats.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Base URL for the site\n",
    "base_url = \"https://www.footballdb.com\"\n",
    "\n",
    "# Categories of stats we need to scrape\n",
    "categories = [\n",
    "    \"passing\", \"rushing\", \"receiving\", \"scoring\",\n",
    "    \"kickoffreturns\", \"puntreturns\", \"punting\",\n",
    "    \"fieldgoals\", \"interceptions\", \"sacks\"\n",
    "]\n",
    "\n",
    "# Create a directory to store the HTML files\n",
    "if not os.path.exists(\"stat_tables\"):\n",
    "    os.makedirs(\"stat_tables\")\n",
    "\n",
    "def scrape_stats(category):\n",
    "    # Construct the URL for the given category for the year 2024 with the limit parameter\n",
    "    url = f\"{base_url}/statistics/nfl/player-stats/{category}/2024/regular-season?sort=defsack&limit=all\"\n",
    "    \n",
    "    # Include headers to mimic a real browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the table with the class `statistics scrollable`\n",
    "        table = soup.find('table', class_='statistics scrollable')\n",
    "        \n",
    "        if table:\n",
    "            table_html = str(table)  # Get the full HTML for the table\n",
    "            \n",
    "            # Save the HTML to a file named after the category\n",
    "            file_name = f\"stat_tables/{category}_stats.html\"\n",
    "            with open(file_name, 'w', encoding='utf-8') as file:\n",
    "                file.write(table_html)\n",
    "            print(f\"Successfully saved stats for {category} to {file_name}\")\n",
    "        else:\n",
    "            print(f\"No table found for {category}.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch {category}: Status code {response.status_code}\")\n",
    "\n",
    "def main():\n",
    "    for category in categories:\n",
    "        scrape_stats(category)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ./csv_output/punting_stats.csv\n",
      "Created ./csv_output/scoring_stats.csv\n",
      "Created ./csv_output/passing_stats.csv\n",
      "Created ./csv_output/puntreturns_stats.csv\n",
      "Created ./csv_output/fieldgoals_stats.csv\n",
      "Created ./csv_output/kickoffreturns_stats.csv\n",
      "Created ./csv_output/sacks_stats.csv\n",
      "Created ./csv_output/interceptions_stats.csv\n",
      "Created ./csv_output/receiving_stats.csv\n",
      "Created ./csv_output/rushing_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_to_csv(html_file, csv_file):\n",
    "    # Read the HTML file\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the table\n",
    "    table = soup.find('table', class_='statistics scrollable')\n",
    "\n",
    "    # Extract headers\n",
    "    headers = [th.text.strip() for th in table.find_all('th')]\n",
    "\n",
    "    # Extract rows\n",
    "    rows = []\n",
    "    for tr in table.find_all('tr')[1:]:  # Skip the header row\n",
    "        row = [td.text.strip() for td in tr.find_all('td')]\n",
    "        # Pad the row with empty strings if it's shorter than the header\n",
    "        row += [''] * (len(headers) - len(row))\n",
    "        rows.append(row[:len(headers)])  # Truncate if longer than header\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "    # Write to CSV\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Created {csv_file}\")\n",
    "\n",
    "def process_all_files(input_dir, output_dir):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each HTML file\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.html'):\n",
    "            html_file = os.path.join(input_dir, filename)\n",
    "            csv_file = os.path.join(output_dir, filename.replace('.html', '.csv'))\n",
    "            try:\n",
    "                html_to_csv(html_file, csv_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "# Usage\n",
    "input_directory = '/home/jose/Code/NFLData/stat_tables'  # Adjust as needed\n",
    "output_directory = './csv_output'\n",
    "process_all_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collects Keys and Their Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: passing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_351680/4127518319.py:36: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  key_b_tag = soup.find('b', text='KEY:')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keys for passing:\n",
      "Gms: Games Played\n",
      "Att: Pass Attempts\n",
      "Cmp: Pass Completions\n",
      "Pct: Pass Completion Percentage\n",
      "Yds: Passing Yards\n",
      "YPA: Yards Per Pass Attempt\n",
      "TD: Touchdown Passes\n",
      "TD%: Touchdown Pass Percentage\n",
      "Int: Intercepted Passes\n",
      "Int%: Pass Interception Percentage\n",
      "Lg: Longest Pass Completion\n",
      "Sack: Passing Sacks\n",
      "Loss: Sack Yards Lost\n",
      "Rate: Passer Rating\n",
      "\n",
      "\n",
      "Processing category: rushing\n",
      "Extracted keys for rushing:\n",
      "Gms: Games Played\n",
      "Att: Rushing Attempts\n",
      "Yds: Rushing Attempts\n",
      "Avg: Rushing Average\n",
      "YPG: Rushing Yards Per Game\n",
      "Lg: Longest Rush\n",
      "TD: Rushing Touchdowns\n",
      "FD: Rushing First Downs\n",
      "\n",
      "\n",
      "Processing category: receiving\n",
      "Extracted keys for receiving:\n",
      "Gms: Games Played\n",
      "Rec: Receptions\n",
      "Yds: Receiving Yards\n",
      "Avg: Receiving Average\n",
      "YPG: Receiving Yards Per Game\n",
      "Lg: Longest Reception\n",
      "TD: Touchdown Receptions\n",
      "FD: First Down Receptions\n",
      "Tar: Receiving Targets\n",
      "YAC: Yards After Catch\n",
      "\n",
      "\n",
      "Processing category: scoring\n",
      "Extracted keys for scoring:\n",
      "Tot: Total Touchdowns\n",
      "R: Rushing Touchdowns\n",
      "P: Touchdown Receptions\n",
      "KR: Kickoff Return Touchdowns\n",
      "PR: Punt Return Touchdowns\n",
      "IR: Interception Return Touchdowns\n",
      "FR: Fumble Return Touchdowns\n",
      "BK: Blocked Field Goal Touchdowns\n",
      "FGR: Missed Field Goal Return Touchdowns\n",
      "PAT: Extra Points Made\n",
      "FG: Field Goals Made\n",
      "Conv: Rushing & Receiving Conversions\n",
      "Saf: Safeties\n",
      "Pts: Points Scored\n",
      "\n",
      "\n",
      "Processing category: kickoffreturns\n",
      "Extracted keys for kickoffreturns:\n",
      "Gms: Games Played\n",
      "Att: Pass Attempts\n",
      "Cmp: Pass Completions\n",
      "Pct: Pass Completion Percentage\n",
      "Yds: Passing Yards\n",
      "YPA: Yards Per Pass Attempt\n",
      "TD: Touchdown Passes\n",
      "TD%: Touchdown Pass Percentage\n",
      "Int: Intercepted Passes\n",
      "Int%: Pass Interception Percentage\n",
      "Lg: Longest Pass Completion\n",
      "Sack: Passing Sacks\n",
      "Loss: Sack Yards Lost\n",
      "Rate: Passer Rating\n",
      "\n",
      "\n",
      "Processing category: puntreturns\n",
      "Extracted keys for puntreturns:\n",
      "Gms: Games Played\n",
      "Att: Pass Attempts\n",
      "Cmp: Pass Completions\n",
      "Pct: Pass Completion Percentage\n",
      "Yds: Passing Yards\n",
      "YPA: Yards Per Pass Attempt\n",
      "TD: Touchdown Passes\n",
      "TD%: Touchdown Pass Percentage\n",
      "Int: Intercepted Passes\n",
      "Int%: Pass Interception Percentage\n",
      "Lg: Longest Pass Completion\n",
      "Sack: Passing Sacks\n",
      "Loss: Sack Yards Lost\n",
      "Rate: Passer Rating\n",
      "\n",
      "\n",
      "Processing category: punting\n",
      "Extracted keys for punting:\n",
      "Punts: Punts\n",
      "Yds: Punting Yards\n",
      "Avg: Gross Punting Average\n",
      "Lg: Longest Punt\n",
      "TB: Touchbacks\n",
      "In20: Punts Inside Opponents 20 Yard Line\n",
      "OB: Punts Out Of Bounds\n",
      "FC: Punts Fair Caught\n",
      "Dwn: Punts Downed\n",
      "Blk: Punts Blocked\n",
      "Net: Net Punting Average\n",
      "Ret: Punts Returned\n",
      "RYds: Punt Return Yards\n",
      "TD: Punt Return Touchdowns\n",
      "\n",
      "\n",
      "Processing category: fieldgoals\n",
      "Extracted keys for fieldgoals:\n",
      "Gms: Games Played\n",
      "Att: Pass Attempts\n",
      "Cmp: Pass Completions\n",
      "Pct: Pass Completion Percentage\n",
      "Yds: Passing Yards\n",
      "YPA: Yards Per Pass Attempt\n",
      "TD: Touchdown Passes\n",
      "TD%: Touchdown Pass Percentage\n",
      "Int: Intercepted Passes\n",
      "Int%: Pass Interception Percentage\n",
      "Lg: Longest Pass Completion\n",
      "Sack: Passing Sacks\n",
      "Loss: Sack Yards Lost\n",
      "Rate: Passer Rating\n",
      "\n",
      "\n",
      "Processing category: interceptions\n",
      "Extracted keys for interceptions:\n",
      "Gms: Games Played\n",
      "Att: Pass Attempts\n",
      "Cmp: Pass Completions\n",
      "Pct: Pass Completion Percentage\n",
      "Yds: Passing Yards\n",
      "YPA: Yards Per Pass Attempt\n",
      "TD: Touchdown Passes\n",
      "TD%: Touchdown Pass Percentage\n",
      "Int: Intercepted Passes\n",
      "Int%: Pass Interception Percentage\n",
      "Lg: Longest Pass Completion\n",
      "Sack: Passing Sacks\n",
      "Loss: Sack Yards Lost\n",
      "Rate: Passer Rating\n",
      "\n",
      "\n",
      "Processing category: sacks\n",
      "Extracted keys for sacks:\n",
      "Gms: Games Played\n",
      "Att: Pass Attempts\n",
      "Cmp: Pass Completions\n",
      "Pct: Pass Completion Percentage\n",
      "Yds: Passing Yards\n",
      "YPA: Yards Per Pass Attempt\n",
      "TD: Touchdown Passes\n",
      "TD%: Touchdown Pass Percentage\n",
      "Int: Intercepted Passes\n",
      "Int%: Pass Interception Percentage\n",
      "Lg: Longest Pass Completion\n",
      "Sack: Passing Sacks\n",
      "Loss: Sack Yards Lost\n",
      "Rate: Passer Rating\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL for the site\n",
    "base_url = \"https://www.footballdb.com\"\n",
    "\n",
    "# Categories of stats we need to scrape\n",
    "categories = [\n",
    "    \"passing\", \"rushing\", \"receiving\", \"scoring\",\n",
    "    \"kickoffreturns\", \"puntreturns\", \"punting\",\n",
    "    \"fieldgoals\", \"interceptions\", \"sacks\"\n",
    "]\n",
    "\n",
    "def fetch_html(url):\n",
    "    \"\"\"\n",
    "    Fetches HTML content from the given URL.\n",
    "    \"\"\"\n",
    "    # Include headers to mimic a real browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\\\n",
    "                       AppleWebKit/537.36 (KHTML, like Gecko)\\\n",
    "                       Chrome/58.0.3029.110 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_keys(soup):\n",
    "    \"\"\"\n",
    "    Extracts only the key-value pairs for passing statistics from the HTML content.\n",
    "    \"\"\"\n",
    "    key_b_tag = soup.find('b', text='KEY:')\n",
    "    key_value_pairs = {}\n",
    "    \n",
    "    if key_b_tag:\n",
    "        key_parent_tag = key_b_tag.parent\n",
    "        key_text = key_parent_tag.get_text(separator=' ')\n",
    "        \n",
    "        # Extract the text between 'KEY:' and the first occurrence of a newline character\n",
    "        key_values_text = key_text.split('KEY:')[1].split('\\n')[0].strip()\n",
    "        pairs = key_values_text.split(',')\n",
    "\n",
    "        for pair in pairs:\n",
    "            if '=' in pair:\n",
    "                key, value = pair.split('=', 1)\n",
    "                key = key.strip().strip('\\u200b')  # Remove any zero-width spaces\n",
    "                value = value.strip()\n",
    "                key_value_pairs[key] = value\n",
    "    else:\n",
    "        print(\"KEY: not found in the HTML content.\")\n",
    "    \n",
    "    return key_value_pairs\n",
    "\n",
    "def scrape_keys_for_category(category):\n",
    "    \"\"\"\n",
    "    Fetches the page for a category and extracts the key-value pairs.\n",
    "    \"\"\"\n",
    "    print(f\"Processing category: {category}\")\n",
    "    # Construct the URL for the given category for the year 2024 with the limit parameter\n",
    "    url = f\"{base_url}/statistics/nfl/player-stats/{category}/2024/regular-season?sort=defsack&limit=all\"\n",
    "\n",
    "    html_content = fetch_html(url)\n",
    "    if not html_content:\n",
    "        print(f\"Failed to retrieve content for {category}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract keys and their descriptions\n",
    "    key_value_pairs = extract_keys(soup)\n",
    "\n",
    "    if key_value_pairs:\n",
    "        print(f\"Extracted keys for {category}:\")\n",
    "        for key, value in key_value_pairs.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"No keys found for {category}\\n\")\n",
    "\n",
    "    return key_value_pairs\n",
    "\n",
    "def main():\n",
    "    all_keys = {}\n",
    "    for category in categories:\n",
    "        keys = scrape_keys_for_category(category)\n",
    "        if keys:\n",
    "            all_keys[category] = keys\n",
    "\n",
    "    # Now all_keys dictionary contains keys for all categories\n",
    "    # You can use this data as needed\n",
    "    # For example, print all keys\n",
    "    # print(all_keys)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
